{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Arxiv Link to original paper: <a href=\"https://arxiv.org/abs/1505.04597\">U-Net: Convolutional Networks for Biomedical Image Segmentation</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pE9h7Hfsull",
        "outputId": "f45ff802-088d-48a7-9766-0400cf5be075"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive',force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kka8-TGyslLK"
      },
      "outputs": [],
      "source": [
        "## Imports\n",
        "import zipfile\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "\n",
        "import matplotlib\n",
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "import matplotlib.image as mpimg\n",
        "import glob\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from sklearn.metrics import roc_curve, auc,roc_auc_score, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from itertools import cycle\n",
        "import seaborn as sns\n",
        "import statistics as st\n",
        "import math\n",
        "import datetime\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "import keras.backend as K\n",
        "import plotly.express as px \n",
        "\n",
        "## Seeding \n",
        "seed = 42\n",
        "#random.seed = seed\n",
        "np.random.seed = seed\n",
        "tf.seed = seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TbyBamTslLK"
      },
      "source": [
        "# 1.4 Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjezXwOfrwGH"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/Elizabeth_PhD_Folder/lfe_images/train.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mngETDmHsVwU"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile('/content/drive/MyDrive/Elizabeth_PhD_Folder/lfe_images/test.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoAgCZNUOebM"
      },
      "source": [
        "##Plotting Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ESRy5WjGONRJ",
        "outputId": "89f14523-3004-4c16-b4ad-ebb1fe3e7f82"
      },
      "outputs": [],
      "source": [
        "# Define some useful functions\n",
        "class PlotLossAccuracy(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.acc = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_acc = []\n",
        "\n",
        "        self.iou = []\n",
        "        self.val_iou = []\n",
        "\n",
        "        self.dice = []\n",
        "        self.val_dice = []\n",
        "\n",
        "        self.logs = []\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.logs.append(logs)\n",
        "        self.x.append(int(self.i))\n",
        "        self.losses.append(logs.get('loss'))\n",
        "        self.val_losses.append(logs.get('val_loss'))\n",
        "        self.acc.append(logs.get('accuracy'))\n",
        "        self.val_acc.append(logs.get('val_accuracy'))\n",
        "\n",
        "        self.iou.append(logs.get('IoU'))\n",
        "        self.val_iou.append(logs.get('val_IoU'))\n",
        "\n",
        "        self.dice.append(logs.get('f1_metric'))\n",
        "        self.val_dice.append(logs.get('val_f1_metric'))\n",
        "        # print(self.acc)\n",
        "        # print(self.val_acc)\n",
        "        # print(logs)\n",
        "        \n",
        "        self.i += 1\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        plt.figure(figsize=(32, 6))\n",
        "       \n",
        "        #plt.plot([1, 4])\n",
        "        plt.subplot(131) \n",
        "        plt.tick_params(axis='both', which='major', labelsize=18)\n",
        "        plt.plot(self.x, self.losses, label=\"train loss\")\n",
        "        plt.plot(self.x, self.val_losses, label=\"validation loss\")\n",
        "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        plt.ylabel('loss',fontsize=25)\n",
        "        plt.xlabel('epoch',fontsize=25)\n",
        "        plt.title('Model Loss',fontsize=28)\n",
        "        plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        plt.legend(fontsize=18)\n",
        "\n",
        "        plt.subplot(132) \n",
        "        plt.tick_params(axis='both', which='major', labelsize=18)        \n",
        "        plt.plot(self.x, self.acc, label=\"training accuracy\")\n",
        "        plt.plot(self.x, self.val_acc, label=\"validation accuracy\")\n",
        "        plt.legend(fontsize=18)\n",
        "        plt.ylabel('accuracy',fontsize=25)\n",
        "        plt.xlabel('epoch',fontsize=25)\n",
        "        plt.title('Model Accuracy',fontsize=28)\n",
        "        plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "\n",
        "        plt.subplot(133)   \n",
        "        plt.tick_params(axis='both', which='major', labelsize=18)      \n",
        "        plt.plot(self.x, self.iou, label=\"training accuracy\")\n",
        "        plt.plot(self.x, self.val_iou, label=\"validation accuracy\")\n",
        "        plt.legend(fontsize=18)\n",
        "        plt.ylabel('IoU',fontsize=25)\n",
        "        plt.xlabel('epoch',fontsize=25)\n",
        "        plt.title('IoU',fontsize=28)\n",
        "        plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "        plt.show();     \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVEpE4E_slLL"
      },
      "source": [
        "## Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wiXNEK1A1nz"
      },
      "outputs": [],
      "source": [
        "class DataGen(keras.utils.Sequence):\n",
        "    def __init__(self, ids, path, batch_size, image_w, image_h):\n",
        "        self.ids = ids\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.image_w = image_w\n",
        "        self.image_h =image_h\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __load__(self, id_name):\n",
        "        ## Path\n",
        "        image_path = os.path.join(self.path, id_name, \"images\", id_name) + \".npy\"\n",
        "        mask_path = os.path.join(self.path, id_name, \"masks/\")\n",
        "        all_masks = os.listdir(mask_path)\n",
        "        \n",
        "        ## Reading Image\n",
        "        image = np.load(image_path, allow_pickle=True)\n",
        "        resize = tf.keras.Sequential([layers.Resizing(self.image_h, self.image_w)])\n",
        "        image = resize(image)\n",
        "        \n",
        "        \n",
        "        ## Reading Masks\n",
        "        for name in all_masks:\n",
        "            _mask_path = mask_path + name\n",
        "            _mask_image = np.load(_mask_path,allow_pickle=True)\n",
        "            _mask_image=np.reshape(_mask_image,(_mask_image.shape[0],_mask_image.shape[1],1))\n",
        "            resize = tf.keras.Sequential([layers.Resizing(self.image_h, self.image_w)])\n",
        "            mask = resize(_mask_image)[:,:,0]\n",
        "            mask=np.where(mask>0, 1, 0).reshape(self.image_h,self.image_w,1)\n",
        "\n",
        "            \n",
        "         \n",
        "            \n",
        "        #Label\n",
        "        label_path = os.path.join(self.path, id_name, \"label\", id_name) + \".npy\"\n",
        "        label = np.load(label_path, allow_pickle=True)\n",
        "\n",
        "        #Trajectory\n",
        "        traj_path = os.path.join(self.path, id_name, \"traj\", id_name) + \".npy\"\n",
        "        traj = np.load(traj_path, allow_pickle=True)\n",
        "        lat_s, lat_m, lt_s, lt_m = traj[0], traj[1], traj[2], traj[3]\n",
        "        lat_s_arr=np.full((self.image_h, self.image_w, 1),lat_s)\n",
        "        lat_m_arr=np.full((self.image_h, self.image_w, 1),lat_m)\n",
        "        lt_s_arr =np.full((self.image_h, self.image_w, 1),lt_s)\n",
        "        lt_m_arr=np.full((self.image_h, self.image_w, 1),lt_m)\n",
        "\n",
        "        #Join\n",
        "        im_all_channels = np.concatenate([image, lat_s_arr, lat_m_arr,lt_s_arr, lt_m_arr], axis=2)\n",
        "\n",
        "        return im_all_channels, mask\n",
        "\n",
        "\n",
        "\n",
        "    def gaussian_noise(self, total_array, mask_array):\n",
        "        flux=total_array[:,:,0:1]\n",
        "        pol=total_array[:,:,1:2]\n",
        "        noise = tf.random.normal(shape=tf.shape(flux), mean=0.0, stddev=0.25,dtype=tf.float32)\n",
        "        flux=flux+noise\n",
        "        flux=np.clip(flux, 0, 1)\n",
        "        pol=pol+noise\n",
        "        pol=np.clip(pol, 0, 1)\n",
        "        \n",
        "        im = tf.concat([flux, pol,total_array[:,:,2:]], axis=2)\n",
        "        im = np.reshape(im, (1, self.image_h,self.image_w, 6))\n",
        "        mask_array = np.reshape(mask_array, (1, self.image_h,self.image_w, 1))\n",
        "      \n",
        "        return im, mask_array\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if(index+1)*self.batch_size > len(self.ids):\n",
        "            self.batch_size = len(self.ids) - index*self.batch_size\n",
        "        \n",
        "        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        \n",
        "        im=[]\n",
        "        mask=[]\n",
        "\n",
        "        for id_name in files_batch:\n",
        "            _im_all_channels, _mask = self.__load__(id_name)\n",
        "            im.append(_im_all_channels)\n",
        "            mask.append(_mask)\n",
        "        \n",
        "        im=np.array(im)\n",
        "        mask=np.array(mask)\n",
        "        \n",
        "        ##Add augmentation\n",
        "        inds=list(np.arange(len(self.ids)))\n",
        "        inds_augment = random.choices(inds, k=3)\n",
        "\n",
        "\n",
        "        #Horizontal Flip\n",
        "        #h_ind = inds_augment[0]\n",
        "        #h_im, h_mask = self.__load__(self.ids[h_ind])\n",
        "        #h_flipped = self.horizontal(h_im, h_mask)\n",
        "        \n",
        "        \n",
        "\n",
        "        #Gaussian Noise\n",
        "        #g_ind = inds_augment[2]\n",
        "        #g_im, g_mask = self.__load__(self.ids[g_ind])\n",
        "        #gaussian_noise = self.gaussian_noise(g_im, g_mask)\n",
        "        \n",
        "\n",
        "        #im = np.concatenate([im],axis=0)\n",
        "        #mask=np.concatenate([mask,gaussian_noise[1]],axis=0)\n",
        "        \n",
        "        \n",
        "        return im, mask\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "    \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.ids)/float(self.batch_size)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy6n2wYSoEI1"
      },
      "outputs": [],
      "source": [
        "class ValidGen(keras.utils.Sequence):\n",
        "    def __init__(self, ids, path, batch_size, image_w, image_h):\n",
        "        self.ids = ids\n",
        "        self.path = path\n",
        "        self.batch_size = batch_size\n",
        "        self.image_w = image_w\n",
        "        self.image_h =image_h\n",
        "        self.on_epoch_end()\n",
        "        \n",
        "    def __load__(self, id_name):\n",
        "        ## Path\n",
        "        image_path = os.path.join(self.path, id_name, \"images\", id_name) + \".npy\"\n",
        "        mask_path = os.path.join(self.path, id_name, \"masks/\")\n",
        "        all_masks = os.listdir(mask_path)\n",
        "\n",
        "        ## Reading Image\n",
        "        image = np.load(image_path, allow_pickle=True)\n",
        "        resize = tf.keras.Sequential([layers.Resizing(self.image_h, self.image_w)])\n",
        "        image = resize(image)\n",
        "\n",
        "        ## Reading Masks\n",
        "        for name in all_masks:\n",
        "            _mask_path = mask_path + name\n",
        "            _mask_image = np.load(_mask_path,allow_pickle=True)\n",
        "            _mask_image=np.reshape(_mask_image,(_mask_image.shape[0],_mask_image.shape[1],1))\n",
        "            resize = tf.keras.Sequential([layers.Resizing(self.image_h, self.image_w)])\n",
        "            mask = resize(_mask_image)[:,:,0]\n",
        "            mask=np.where(mask>0, 1, 0).reshape(self.image_h,self.image_w,1)\n",
        "\n",
        "        #Label\n",
        "        label_path = os.path.join(self.path, id_name, \"label\", id_name) + \".npy\"\n",
        "        label = np.load(label_path, allow_pickle=True)\n",
        "\n",
        "        #trajectory\n",
        "        traj_path = os.path.join(self.path, id_name, \"traj\", id_name) + \".npy\"\n",
        "        traj = np.load(traj_path, allow_pickle=True)\n",
        "        lat_s, lat_m, lt_s, lt_m = traj[0], traj[1], traj[2], traj[3]\n",
        "        lat_s_arr=np.full((self.image_h, self.image_w, 1),lat_s)\n",
        "        lat_m_arr=np.full((self.image_h, self.image_w, 1),lat_m)\n",
        "        lt_s_arr =np.full((self.image_h, self.image_w, 1),lt_s)\n",
        "        lt_m_arr=np.full((self.image_h, self.image_w, 1),lt_m)\n",
        "\n",
        "        im_all_channels = np.concatenate([image, lat_s_arr, lat_m_arr,lt_s_arr, lt_m_arr], axis=2)\n",
        "\n",
        "        return im_all_channels, mask\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if(index+1)*self.batch_size > len(self.ids):\n",
        "            self.batch_size = len(self.ids) - index*self.batch_size\n",
        "        \n",
        "        files_batch = self.ids[index*self.batch_size : (index+1)*self.batch_size]\n",
        "        \n",
        "        im=[]\n",
        "        mask=[]\n",
        "        \n",
        "        for id_name in files_batch:\n",
        "            _im_all_channels, _mask = self.__load__(id_name)\n",
        "            im.append(_im_all_channels)\n",
        "            mask.append(_mask)\n",
        "        \n",
        "        im=np.array(im)\n",
        "        mask=np.array(mask)\n",
        "        return im, mask\n",
        "    \n",
        "    def on_epoch_end(self):\n",
        "        pass\n",
        "    \n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.ids)/float(self.batch_size)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d238Zip_slLN"
      },
      "source": [
        "## UNet Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSOknZk-slLN"
      },
      "source": [
        "## Different Convolutional Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lULxMsxqslLM"
      },
      "outputs": [],
      "source": [
        "train_path = \"/content/train/\"\n",
        "epochs = 60\n",
        "batch_size = 16\n",
        "f = [64,128, 256, 512,1024]\n",
        "\n",
        "\n",
        "## Training Ids\n",
        "random_ids = next(os.walk(train_path))[1]\n",
        "\n",
        "count=3000\n",
        "ids=[str(i).zfill(3) for i in np.arange(count)]\n",
        "total_ids = [i for i in ids if i in random_ids]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Validation Data Size\n",
        "#Train set is 75% of total, and validation is 1/3 of the train set and so it is 25% of total data\n",
        "train_label=np.load('/content/drive/MyDrive/Elizabeth_PhD_Folder/lfe_images/train_label.npy',allow_pickle=True)\n",
        "train_ids, valid_ids = train_test_split(total_ids, test_size=.35, random_state=42,stratify=train_label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9ChUzn6slLN"
      },
      "outputs": [],
      "source": [
        "def down_block(x, filters, do,kernel_size=(3, 3), padding='same', strides=1):\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    dp1 = keras.layers.Dropout(do)(c)\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(dp1)\n",
        "    p = keras.layers.MaxPool2D((2, 2), (2, 2))(c)\n",
        "\n",
        "    return c, p\n",
        "\n",
        "def up_block(x, skip, filters, do,kernel_size=(3,3), padding='same', strides=1):\n",
        "    us = keras.layers.UpSampling2D((2, 2))(x)\n",
        "    concat = keras.layers.Concatenate()([us, skip])\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(concat)\n",
        "    dp1 = keras.layers.Dropout(do)(c)\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(dp1)\n",
        "    \n",
        "    return c\n",
        "\n",
        "def bottleneck(x, filters, do,kernel_size=(3,3), padding='same', strides=1):\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(x)\n",
        "    dp1 = keras.layers.Dropout(do)(c)\n",
        "    c = keras.layers.Conv2D(filters, kernel_size, padding=padding, strides=strides, activation=\"relu\")(dp1)\n",
        "    return c"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRuNaiz0slLN"
      },
      "outputs": [],
      "source": [
        "def UNet(do):\n",
        "    inputs = keras.layers.Input((image_h, image_w, channels))\n",
        "    \n",
        "    p0 = inputs\n",
        "    c1, p1 = down_block(p0, f[0],do) \n",
        "    c2, p2 = down_block(p1, f[1],do) \n",
        "    c3, p3 = down_block(p2, f[2],do) \n",
        "    c4, p4 = down_block(p3, f[3],do) \n",
        "    \n",
        "    bn = bottleneck(p4, f[4],do)\n",
        "    \n",
        "    u4 = up_block(bn, c4, f[3],do) \n",
        "    u5 = up_block(u4, c3, f[2],do) \n",
        "    u6 = up_block(u5, c2, f[1],do) \n",
        "    u7 = up_block(u6, c1, f[0],do) \n",
        "    \n",
        "    \n",
        "    outputs = keras.layers.Conv2D(1, (1, 1), padding=\"same\", activation=\"sigmoid\")(u7)\n",
        "    model = keras.models.Model(inputs, outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7ytYzVQslLM"
      },
      "source": [
        "## Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh1uaSMOrc2O"
      },
      "outputs": [],
      "source": [
        "def dice_coef(y_true, y_pred, smooth=100):        \n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    dice = (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
        "    return dice\n",
        "\n",
        "def dice_coef_loss(y_true, y_pred):\n",
        "    y_true=tf.cast(y_true, dtype=tf.float32)\n",
        "    return 1 - dice_coef(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drHLCYOD-kZ2",
        "outputId": "ec56a6e3-b158-4c14-ab56-2528d4927234"
      },
      "outputs": [],
      "source": [
        "image_w = 128\n",
        "image_h = 384\n",
        "channels=6\n",
        "do=0.4\n",
        "model = UNet(do)\n",
        "optname='Adam'\n",
        "lr =1e-4\n",
        "opt=tf.keras.optimizers.Adam(learning_rate=lr,name=optname)\n",
        "binary_iou= tf.keras.metrics.BinaryIoU(name='IoU')\n",
        "model.summary()\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy',\n",
        "              metrics=['accuracy', binary_iou])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eAlELiGslLO"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40h3FYlnNbzp",
        "outputId": "cd1ffbfb-8570-4898-da12-fb13b3c58c54"
      },
      "outputs": [],
      "source": [
        "root = '/content/drive/MyDrive/Elizabeth_PhD_Folder/Colab_Notebooks/Models/'\n",
        "max_filter=f[-1]\n",
        "model_label = 'UNET_'+str(optname) + 'LR' +str(lr)+'_6steps_minfilter'+str(f[0])+'maxfilter'+str(max_filter)+'_binaryloss'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulw_71s_AHMN"
      },
      "outputs": [],
      "source": [
        "train_gen = DataGen(train_ids, train_path, image_h=image_h, image_w=image_w, batch_size=batch_size)\n",
        "valid_gen = ValidGen(valid_ids, train_path, image_h=image_h, image_w=image_w, batch_size=batch_size)\n",
        "\n",
        "\n",
        "train_steps = len(train_ids)//(batch_size)\n",
        "valid_steps = len(valid_ids)//(batch_size)\n",
        "\n",
        "\n",
        "pltCallBack = PlotLossAccuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        },
        "id": "OxkZgDQgslLO",
        "outputId": "93bbaca5-5974-4f08-bcd9-1c12a8c42afe",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "root = '/content/drive/MyDrive/Elizabeth_PhD_Folder/Colab_Notebooks/Models/'\n",
        "checkpoint_filepath = root+model_label\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,save_weights_only=False,monitor='val_accuracy',mode='auto',save_best_only=False)\n",
        "#Fit Model\n",
        "epochs=60\n",
        "history = model.fit(train_gen, validation_data=valid_gen, steps_per_epoch=train_steps, validation_steps=valid_steps, epochs=epochs,verbose=1, callbacks=[model_checkpoint_callback,pltCallBack])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
